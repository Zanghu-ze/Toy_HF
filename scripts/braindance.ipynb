{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "braindance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgIaFMaff4Rb"
      },
      "source": [
        "# Geometry-Free View Synthesis\n",
        "\n",
        "This is a colab demo for [Geometry-Free View Synthesis](https://github.com/CompVis/geometry-free-view-synthesis). Compared to [the pygame demo](https://github.com/CompVis/geometry-free-view-synthesis#demo) the controls of this one are a bit clumsy. But you can dive right in by selecting `Runtime->Run all`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN7hjuNKcnT0"
      },
      "source": [
        "Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAm5fMd3avVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d547c984-650e-4020-bdf9-b95166c2008e"
      },
      "source": [
        "!pip install git+https://github.com/CompVis/geometry-free-view-synthesis#egg=geometry-free-view-synthesis"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geometry-free-view-synthesis\n",
            "  Cloning https://github.com/CompVis/geometry-free-view-synthesis to /tmp/pip-install-be1z0gp1/geometry-free-view-synthesis_e8fcca15d1ef466e868890c799763668\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/CompVis/geometry-free-view-synthesis /tmp/pip-install-be1z0gp1/geometry-free-view-synthesis_e8fcca15d1ef466e868890c799763668\n",
            "  Resolved https://github.com/CompVis/geometry-free-view-synthesis to commit 00dc639c98dfb9246bee0009649c5be8f8b58e1e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting splatting@ git+https://github.com/pesser/splatting@1427d7c4204282d117403b35698d489e0324287f#egg=splatting (from geometry-free-view-synthesis)\n",
            "  Using cached splatting-0.0.0-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (0.20.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (4.66.6)\n",
            "Requirement already satisfied: omegaconf>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (2.3.0)\n",
            "Requirement already satisfied: pytorch-lightning>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (2.4.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (2.6.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (0.8.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (6.4.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (2.36.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (0.5.1)\n",
            "Requirement already satisfied: test-tube in /usr/local/lib/python3.10/dist-packages (from geometry-free-view-synthesis) (0.7.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0.0->geometry-free-view-synthesis) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0.0->geometry-free-view-synthesis) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (0.11.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->geometry-free-view-synthesis) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->geometry-free-view-synthesis) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->geometry-free-view-synthesis) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->geometry-free-view-synthesis) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->geometry-free-view-synthesis) (1.3.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->geometry-free-view-synthesis) (11.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->geometry-free-view-synthesis) (75.1.0)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from test-tube->geometry-free-view-synthesis) (2.2.2)\n",
            "Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from test-tube->geometry-free-view-synthesis) (2.17.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from test-tube->geometry-free-view-synthesis) (1.0.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (3.10.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube->geometry-free-view-synthesis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube->geometry-free-view-synthesis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube->geometry-free-view-synthesis) (2024.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (4.25.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube->geometry-free-view-synthesis) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->geometry-free-view-synthesis) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.0.8->geometry-free-view-synthesis) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ValMhAickzc"
      },
      "source": [
        "Image loading function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9YgbhaMbwCq"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def load_im_as_example(im):\n",
        "    size = [208, 368]\n",
        "    w,h = im.size\n",
        "    if np.abs(w/h - size[1]/size[0]) > 0.1:\n",
        "        print(f\"Center cropping image to AR {size[1]/size[0]}\")\n",
        "        if w/h < size[1]/size[0]:\n",
        "            # crop h\n",
        "            left = 0\n",
        "            right = w\n",
        "            top = h/2 - size[0]/size[1]*w/2\n",
        "            bottom = h/2 + size[0]/size[1]*w/2\n",
        "        else:\n",
        "            # crop w\n",
        "            top = 0\n",
        "            bottom = h\n",
        "            left = w/2 - size[1]/size[0]*h\n",
        "            right = w/2 + size[1]/size[0]*h\n",
        "        im = im.crop(box=(left, top, right, bottom))\n",
        "\n",
        "    im = im.resize((size[1],size[0]),\n",
        "                   resample=Image.LANCZOS)\n",
        "    im = np.array(im)/127.5-1.0\n",
        "    im = im.astype(np.float32)\n",
        "\n",
        "    example = dict()\n",
        "    example[\"src_img\"] = im\n",
        "    example[\"K\"] = np.array([[184.0, 0.0, 184.0],\n",
        "                             [0.0, 184.0, 104.0],\n",
        "                             [0.0, 0.0, 1.0]], dtype=np.float32)\n",
        "    example[\"K_inv\"] = np.linalg.inv(example[\"K\"])\n",
        "\n",
        "    ## dummy data not used during inference\n",
        "    example[\"dst_img\"] = np.zeros_like(example[\"src_img\"])\n",
        "    example[\"src_points\"] = np.zeros((1,3), dtype=np.float32)\n",
        "\n",
        "    return example\n",
        "\n",
        "def load_as_example(path):\n",
        "    im = Image.open(path)\n",
        "    return load_im_as_example(im)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzcR5qRLcaBw"
      },
      "source": [
        "Define some functions related to the camera control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXlGobz5brB8"
      },
      "source": [
        "def normalize(x):\n",
        "    return x/np.linalg.norm(x)\n",
        "\n",
        "def cosd(x):\n",
        "    return np.cos(np.deg2rad(x))\n",
        "\n",
        "def sind(x):\n",
        "    return np.sin(np.deg2rad(x))\n",
        "\n",
        "def look_to(camera_pos, camera_dir, camera_up):\n",
        "  camera_right = normalize(np.cross(camera_up, camera_dir))\n",
        "  R = np.zeros((4, 4))\n",
        "  R[0,0:3] = normalize(camera_right)\n",
        "  R[1,0:3] = normalize(np.cross(camera_dir, camera_right))\n",
        "  R[2,0:3] = normalize(camera_dir)\n",
        "  R[3,3] = 1\n",
        "  trans_matrix = np.array([[1.0, 0.0, 0.0, -camera_pos[0]],\n",
        "                           [0.0, 1.0, 0.0, -camera_pos[1]],\n",
        "                           [0.0, 0.0, 1.0, -camera_pos[2]],\n",
        "                           [0.0, 0.0, 0.0,            1.0]])\n",
        "  tmp = R@trans_matrix\n",
        "  return tmp[:3,:3], tmp[:3,3]\n",
        "\n",
        "def rotate_around_axis(angle, axis):\n",
        "    axis = normalize(axis)\n",
        "    rotation = np.array([[cosd(angle)+axis[0]**2*(1-cosd(angle)),\n",
        "                          axis[0]*axis[1]*(1-cosd(angle))-axis[2]*sind(angle),\n",
        "                          axis[0]*axis[2]*(1-cosd(angle))+axis[1]*sind(angle)],\n",
        "                         [axis[1]*axis[0]*(1-cosd(angle))+axis[2]*sind(angle),\n",
        "                          cosd(angle)+axis[1]**2*(1-cosd(angle)),\n",
        "                          axis[1]*axis[2]*(1-cosd(angle))-axis[0]*sind(angle)],\n",
        "                         [axis[2]*axis[0]*(1-cosd(angle))-axis[1]*sind(angle),\n",
        "                          axis[2]*axis[1]*(1-cosd(angle))+axis[0]*sind(angle),\n",
        "                          cosd(angle)+axis[2]**2*(1-cosd(angle))]])\n",
        "    return rotation"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPd6dlYtc01X"
      },
      "source": [
        "Forward splatting of an image given its depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1iYAo5Zc3sw"
      },
      "source": [
        "import torch\n",
        "from splatting import splatting_function\n",
        "\n",
        "def render_forward(src_ims, src_dms,\n",
        "                   Rcam, tcam,\n",
        "                   K_src,\n",
        "                   K_dst):\n",
        "    Rcam = Rcam.to(device=src_ims.device)[None]\n",
        "    tcam = tcam.to(device=src_ims.device)[None]\n",
        "\n",
        "    R = Rcam\n",
        "    t = tcam[...,None]\n",
        "    K_src_inv = K_src.inverse()\n",
        "\n",
        "    assert len(src_ims.shape) == 4\n",
        "    assert len(src_dms.shape) == 3\n",
        "    assert src_ims.shape[1:3] == src_dms.shape[1:3], (src_ims.shape,\n",
        "                                                      src_dms.shape)\n",
        "\n",
        "    x = np.arange(src_ims[0].shape[1])\n",
        "    y = np.arange(src_ims[0].shape[0])\n",
        "    coord = np.stack(np.meshgrid(x,y), -1)\n",
        "    coord = np.concatenate((coord, np.ones_like(coord)[:,:,[0]]), -1) # z=1\n",
        "    coord = coord.astype(np.float32)\n",
        "    coord = torch.as_tensor(coord, dtype=K_src.dtype, device=K_src.device)\n",
        "    coord = coord[None] # bs, h, w, 3\n",
        "\n",
        "    D = src_dms[:,:,:,None,None]\n",
        "\n",
        "    points = K_dst[None,None,None,...]@(R[:,None,None,...]@(D*K_src_inv[None,None,None,...]@coord[:,:,:,:,None])+t[:,None,None,:,:])\n",
        "    points = points.squeeze(-1)\n",
        "\n",
        "    new_z = points[:,:,:,[2]].clone().permute(0,3,1,2) # b,1,h,w\n",
        "    points = points/torch.clamp(points[:,:,:,[2]], 1e-8, None)\n",
        "\n",
        "    src_ims = src_ims.permute(0,3,1,2)\n",
        "    flow = points - coord\n",
        "    flow = flow.permute(0,3,1,2)[:,:2,...]\n",
        "\n",
        "    alpha = 0.5\n",
        "    importance = alpha/new_z\n",
        "    importance_min = importance.amin((1,2,3),keepdim=True)\n",
        "    importance_max = importance.amax((1,2,3),keepdim=True)\n",
        "    importance=(importance-importance_min)/(importance_max-importance_min+1e-6)*10-10\n",
        "    importance = importance.exp()\n",
        "\n",
        "    input_data = torch.cat([importance*src_ims, importance], 1)\n",
        "    output_data = splatting_function(\"summation\", input_data, flow)\n",
        "\n",
        "    num = torch.sum(output_data[:,:-1,:,:], dim=0, keepdim=True)\n",
        "    nom = torch.sum(output_data[:,-1:,:,:], dim=0, keepdim=True)\n",
        "\n",
        "    rendered = num/(nom+1e-7)\n",
        "    rendered = rendered.permute(0,2,3,1)[0,...]\n",
        "    return rendered"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9grU0flhdbbQ"
      },
      "source": [
        "Helper class to render with GeoGPT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgS9UpzMdQGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "67b3e43a-6b2c-4cef-c709-32d3597073e3"
      },
      "source": [
        "from geofree import pretrained_models\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "class Renderer(object):\n",
        "    def __init__(self, model, device):\n",
        "        self.model = pretrained_models(model=model)\n",
        "        self.model = self.model.to(device=device)\n",
        "        self._active = False\n",
        "\n",
        "    def init(self,\n",
        "             start_im,\n",
        "             example,\n",
        "             show_R,\n",
        "             show_t):\n",
        "        self._active = True\n",
        "        self.step = 0\n",
        "\n",
        "        batch = self.batch = default_collate([example])\n",
        "        batch[\"R_rel\"] = show_R[None,...]\n",
        "        batch[\"t_rel\"] = show_t[None,...]\n",
        "\n",
        "        _, cdict, edict = self.model.get_xce(batch)\n",
        "        for k in cdict:\n",
        "            cdict[k] = cdict[k].to(device=self.model.device)\n",
        "        for k in edict:\n",
        "            edict[k] = edict[k].to(device=self.model.device)\n",
        "\n",
        "        quant_d, quant_c, dc_indices, embeddings = self.model.get_normalized_c(cdict,edict,fixed_scale=True)\n",
        "\n",
        "        start_im = start_im[None,...].to(self.model.device).permute(0,3,1,2)\n",
        "        quant_c, c_indices = self.model.encode_to_c(c=start_im)\n",
        "        cond_rec = self.model.cond_stage_model.decode(quant_c)\n",
        "\n",
        "        self.current_im = cond_rec.permute(0,2,3,1)[0]\n",
        "        self.current_sample = c_indices\n",
        "\n",
        "        self.quant_c = quant_c # to know shape\n",
        "        # for sampling\n",
        "        self.dc_indices = dc_indices\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.step < self.current_sample.shape[1]:\n",
        "            z_start_indices = self.current_sample[:, :self.step]\n",
        "            temperature=None\n",
        "            top_k=250\n",
        "            callback=None\n",
        "            index_sample = self.model.sample(z_start_indices, self.dc_indices,\n",
        "                                             steps=1,\n",
        "                                             temperature=temperature if temperature is not None else 1.0,\n",
        "                                             sample=True,\n",
        "                                             top_k=top_k if top_k is not None else 100,\n",
        "                                             callback=callback if callback is not None else lambda k: None,\n",
        "                                             embeddings=self.embeddings)\n",
        "            self.current_sample = torch.cat((index_sample,\n",
        "                                             self.current_sample[:,self.step+1:]),\n",
        "                                            dim=1)\n",
        "\n",
        "            sample_dec = self.model.decode_to_img(self.current_sample,\n",
        "                                                  self.quant_c.shape)\n",
        "            self.current_im = sample_dec.permute(0,2,3,1)[0]\n",
        "            self.step += 1\n",
        "\n",
        "        if self.step >= self.current_sample.shape[1]:\n",
        "            self._active = False\n",
        "\n",
        "        return self.current_im\n",
        "\n",
        "    def active(self):\n",
        "        return self._active\n",
        "\n",
        "    def reconstruct(self, x):\n",
        "        x = x.to(self.model.device).permute(0,3,1,2)\n",
        "        quant_c, c_indices = self.model.encode_to_c(c=x)\n",
        "        x_rec = self.model.cond_stage_model.decode(quant_c)\n",
        "        return x_rec.permute(0,2,3,1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_lightning.utilities.distributed'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7168fbdd3b33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeofree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretrained_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/geofree/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeofree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretrained_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/geofree/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeofree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeogpt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/geofree/models/transformers/geogpt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeofree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeofree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmidas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMidas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/geofree/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearningRateMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning.utilities.distributed'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKVvbIMcgHD"
      },
      "source": [
        "Everything is defined. Load included example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvxlLaA2bB__"
      },
      "source": [
        "import importlib.resources as pkg_resources\n",
        "with pkg_resources.path(\"geofree.examples\", \"artist.jpg\") as path:\n",
        "  example = load_as_example(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERMV5ddyecM0"
      },
      "source": [
        "Initialize models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7N27V4YeGPW"
      },
      "source": [
        "from geofree.modules.warp.midas import Midas\n",
        "\n",
        "model = \"re_impl_depth\"\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Warning: Running on CPU---sampling might take a while...\")\n",
        "    device = torch.device(\"cpu\")\n",
        "midas = Midas().eval().to(device)\n",
        "renderer = Renderer(model=model, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f088sOjex9M"
      },
      "source": [
        "Backend for interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7HI7r0Fo0V"
      },
      "source": [
        "class Looper(object):\n",
        "    def __init__(self, midas, renderer, example):\n",
        "        self.midas = midas\n",
        "        self.renderer = renderer\n",
        "        self.init_example(example)\n",
        "        self.RENDERING = False\n",
        "\n",
        "    def init_example(self, example):\n",
        "        self.example = example\n",
        "\n",
        "        ims = example[\"src_img\"][None,...]\n",
        "        K = example[\"K\"]\n",
        "\n",
        "        # compute depth for preview\n",
        "        dms = [None]\n",
        "        for i in range(ims.shape[0]):\n",
        "            midas_in = torch.tensor(ims[i])[None,...].permute(0,3,1,2).to(device)\n",
        "            scaled_idepth = self.midas.fixed_scale_depth(midas_in, return_inverse_depth=True)\n",
        "            dms[i] = 1.0/scaled_idepth[0].cpu().numpy()\n",
        "\n",
        "        # now switch to pytorch\n",
        "        src_ims = torch.tensor(ims, dtype=torch.float32)\n",
        "        src_dms = torch.tensor(dms, dtype=torch.float32)\n",
        "        K = torch.tensor(K, dtype=torch.float32)\n",
        "\n",
        "        self.src_ims = src_ims.to(device=device)\n",
        "        self.src_dms = src_dms.to(device=device)\n",
        "        self.K = K.to(device=device)\n",
        "\n",
        "        self.init_cam()\n",
        "\n",
        "    def init_cam(self):\n",
        "        self.camera_pos = np.array([0.0, 0.0, 0.0])\n",
        "        self.camera_dir = np.array([0.0, 0.0, 1.0])\n",
        "        self.camera_up = np.array([0.0, 1.0, 0.0])\n",
        "        self.CAM_SPEED = 0.25\n",
        "        self.MOUSE_SENSITIVITY = 10.0\n",
        "\n",
        "    def update_camera(self, keys):\n",
        "        ######### Camera\n",
        "        if keys[\"a\"]:\n",
        "            self.camera_pos += self.CAM_SPEED*normalize(np.cross(self.camera_dir, self.camera_up))\n",
        "        if keys[\"d\"]:\n",
        "            self.camera_pos -= self.CAM_SPEED*normalize(np.cross(self.camera_dir, self.camera_up))\n",
        "        if keys[\"w\"]:\n",
        "            self.camera_pos += self.CAM_SPEED*normalize(self.camera_dir)\n",
        "        if keys[\"s\"]:\n",
        "            self.camera_pos -= self.CAM_SPEED*normalize(self.camera_dir)\n",
        "        if keys[\"q\"]:\n",
        "            self.camera_pos -= self.CAM_SPEED*normalize(self.camera_up)\n",
        "        if keys[\"e\"]:\n",
        "            self.camera_pos += self.CAM_SPEED*normalize(self.camera_up)\n",
        "\n",
        "        camera_yaw = 0\n",
        "        camera_pitch = 0\n",
        "        if \"look\" in keys:\n",
        "            dx, dy = keys[\"look\"]\n",
        "            if not self.RENDERING:\n",
        "                camera_yaw -= self.MOUSE_SENSITIVITY*dx\n",
        "                camera_pitch += self.MOUSE_SENSITIVITY*dy\n",
        "\n",
        "        # adjust for yaw and pitch\n",
        "        rotation = np.array([[cosd(-camera_yaw), 0.0, sind(-camera_yaw)],\n",
        "                             [0.0, 1.0, 0.0],\n",
        "                             [-sind(-camera_yaw), 0.0, cosd(-camera_yaw)]])\n",
        "        self.camera_dir = rotation@self.camera_dir\n",
        "\n",
        "        rotation = rotate_around_axis(camera_pitch, np.cross(self.camera_dir,\n",
        "                                                             self.camera_up))\n",
        "        self.camera_dir = rotation@self.camera_dir\n",
        "\n",
        "        show_R, show_t = look_to(self.camera_pos, self.camera_dir, self.camera_up) # look from pos in direction dir\n",
        "        show_R = torch.as_tensor(show_R, dtype=torch.float32)\n",
        "        show_t = torch.as_tensor(show_t, dtype=torch.float32)\n",
        "\n",
        "        self.show_R = show_R\n",
        "        self.show_t = show_t\n",
        "\n",
        "    def update(self, keys):\n",
        "        self.update_camera(keys)\n",
        "        if not self.RENDERING:\n",
        "            wrp_im = render_forward(self.src_ims, self.src_dms,\n",
        "                                    self.show_R, self.show_t,\n",
        "                                    K_src=self.K,\n",
        "                                    K_dst=self.K)\n",
        "\n",
        "        if keys[\"render\"]:\n",
        "            self.RENDERING = True\n",
        "            self.renderer.init(wrp_im, self.example, self.show_R, self.show_t)\n",
        "\n",
        "        if self.RENDERING:\n",
        "            wrp_im = self.renderer()\n",
        "\n",
        "        if not self.renderer._active or keys[\"stop\"]:\n",
        "          self.RENDERING = False\n",
        "\n",
        "        return wrp_im, self.RENDERING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFHXmjkphpAZ"
      },
      "source": [
        "Frontend inspired by [Infinite Nature](https://infinite-nature.github.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXq8YBdwyTQ8"
      },
      "source": [
        "import IPython\n",
        "from google.colab import output, files\n",
        "import base64\n",
        "import io\n",
        "from io import BytesIO\n",
        "\n",
        "looper = Looper(midas, renderer, example)\n",
        "\n",
        "def as_png(x):\n",
        "  if hasattr(x, \"detach\"):\n",
        "      x = x.detach().cpu().numpy()\n",
        "  #x = x.transpose(1,0,2)\n",
        "  x = (x+1.0)*127.5\n",
        "  x = x.clip(0, 255).astype(np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  Image.fromarray(x).save(data, format=\"png\")\n",
        "  data.seek(0)\n",
        "  data = data.read()\n",
        "  return base64.b64encode(data).decode()\n",
        "\n",
        "def pyloop(data):\n",
        "  if data.get(\"upload\", False):\n",
        "    data = files.upload()\n",
        "    fname = sorted(data.keys())[0]\n",
        "    I = Image.open(BytesIO(data[fname]))\n",
        "    looper.init_example(load_im_as_example(I))\n",
        "\n",
        "  keys = dict()\n",
        "  if \"look\" in data:\n",
        "    keys[\"look\"] = np.array(data[\"look\"])*2.0-1.0\n",
        "  move = data.get(\"direction\", None)\n",
        "  keys[\"w\"] = move==\"forward\"\n",
        "  keys[\"a\"] = move==\"left\"\n",
        "  keys[\"s\"] = move==\"backward\"\n",
        "  keys[\"d\"] = move==\"right\"\n",
        "  keys[\"q\"] = move==\"up\"\n",
        "  keys[\"e\"] = move==\"down\"\n",
        "  keys[\"render\"] = move==\"render\"\n",
        "  keys[\"stop\"] = data.get(\"stop\", False)\n",
        "  output, rendering = looper.update(keys)\n",
        "\n",
        "  ret = dict()\n",
        "  ret[\"image\"] = as_png(output)\n",
        "  ret[\"loop\"] = rendering\n",
        "  ret = IPython.display.JSON(ret)\n",
        "\n",
        "  return ret\n",
        "\n",
        "output.register_callback('pyloop', pyloop)\n",
        "\n",
        "# The front-end for our interactive demo.\n",
        "\n",
        "html='''\n",
        "<style>\n",
        "#view {\n",
        "  width: 368px;\n",
        "  height: 208px;\n",
        "  background-color: #aaa;\n",
        "  background-size: 100% 100%;\n",
        "  border: 1px solid #000;\n",
        "  margin: 20px;\n",
        "  position: relative;\n",
        "}\n",
        ".buttons {\n",
        "  margin: 20px;\n",
        "}\n",
        ".buttons div {\n",
        "  display: inline-block;\n",
        "  cursor: pointer;\n",
        "  padding: 20px;\n",
        "  background: #eee;\n",
        "  border: 2px solid #aaa;\n",
        "  border-radius: 3px;\n",
        "  margin-right: 10px;\n",
        "  font-weight: bold;\n",
        "  text-transform: uppercase;\n",
        "  letter-spacing: 1px;\n",
        "  color: #444;\n",
        "  width: 100px;\n",
        "  text-align: center;\n",
        "}\n",
        ".buttons div:active {\n",
        "  background: #444;\n",
        "  color: #fff;\n",
        "}\n",
        "#rgb {\n",
        "  height: 100%;\n",
        "}\n",
        "h3 {\n",
        "  margin-left: 20px;\n",
        "}\n",
        "</style>\n",
        "<h3>Braindance Colab Demo</h3>\n",
        "<div id=view><img id=rgb></div>\n",
        "<div class=buttons>\n",
        "<div id=up>Up</div><div id=forward>Forward</div><div id=down>Down</div><br>\n",
        "<div id=left>Left</div><div id=backward>Backward</div><div id=right>Right</div><br>\n",
        "<div id=render>Render</div><div id=stop>Stop</div><div id=upload>Upload</div>\n",
        "<script>\n",
        "stop_rendering = false;\n",
        "\n",
        "async function loop(...parms) {\n",
        "  result = await google.colab.kernel.invokeFunction('pyloop', parms, {});\n",
        "  result = result.data['application/json'];\n",
        "  image = result['image'];\n",
        "  // console.log(image);\n",
        "  const url = `data:image/png;base64,${image}`;\n",
        "  document.querySelector('#rgb').src = url;\n",
        "  if(stop_rendering) {\n",
        "    result['loop'] = false;\n",
        "    await google.colab.kernel.invokeFunction('pyloop', [{\"stop\": true}], {});\n",
        "  }\n",
        "  stop_rendering = false;\n",
        "  if(result['loop']) {\n",
        "    loop({});\n",
        "  }\n",
        "}\n",
        "\n",
        "function cursor(e) {\n",
        "  x = e.offsetX / e.target.clientWidth;\n",
        "  y = e.offsetY / e.target.clientHeight;\n",
        "  loop({\"look\": [x,y]})\n",
        "}\n",
        "\n",
        "function move(direction) {\n",
        "  loop({\"direction\": direction})\n",
        "}\n",
        "\n",
        "loop({});\n",
        "document.querySelector('#view').addEventListener('click', cursor);\n",
        "document.querySelector('#up').addEventListener('click', () => move(\"up\"));\n",
        "document.querySelector('#forward').addEventListener('click', () => move(\"forward\"));\n",
        "document.querySelector('#down').addEventListener('click', () => move(\"down\"));\n",
        "document.querySelector('#left').addEventListener('click', () => move(\"left\"));\n",
        "document.querySelector('#backward').addEventListener('click', () => move(\"backward\"));\n",
        "document.querySelector('#right').addEventListener('click', () => move(\"right\"));\n",
        "document.querySelector('#render').addEventListener('click', () => move(\"render\"));\n",
        "document.querySelector('#stop').addEventListener('click', () => {stop_rendering=true;});\n",
        "document.querySelector('#upload').addEventListener('click', () => loop({\"upload\": true}));\n",
        "\n",
        "\n",
        "\n",
        "</script>\n",
        "'''\n",
        "\n",
        "display(IPython.display.HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpdGtv-WhWBw"
      },
      "source": [
        "Click on the image to look around. Use the first six buttons to move around. Render to start rendering the novel view with GeoGPT. Stop to abort rendering and regain control. Upload to upload your own images. Have fun!"
      ]
    }
  ]
}